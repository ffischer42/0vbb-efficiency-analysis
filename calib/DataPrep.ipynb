{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"v07.01\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"src/init.jl\")\n",
    "include(\"src/fct.jl\")\n",
    "data_type = \"cal\"\n",
    "dataset = \"v07.01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sort_data_all_01-new.jl\n",
    "For several runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Run 53 | file 1 of total 4344\n",
      "└ @ Main In[16]:43\n",
      "┌ Info: Estimated time for this file: 221.5 min\n",
      "└ @ Main In[16]:53\n",
      "┌ Info: Estimated time till completion: 1.9 d\n",
      "└ @ Main In[16]:70\n",
      "┌ Info: ------------------------------\n",
      "└ @ Main In[16]:72\n",
      "┌ Info: Load Tier4\n",
      "└ @ Main In[16]:76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25.570073 seconds (4.95 M allocations: 3.452 GiB, 1.72% gc time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Number of events: 833103\n",
      "└ @ Main In[16]:86\n",
      "┌ Info: Number of events after filter: 655657\n",
      "└ @ Main In[16]:87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66-element Array{Any,1}:\n",
       " [1, 10000]\n",
       " [10001, 20000]\n",
       " [20001, 30000]\n",
       " [30001, 40000]\n",
       " [40001, 50000]\n",
       " [50001, 60000]\n",
       " [60001, 70000]\n",
       " [70001, 80000]\n",
       " [80001, 90000]\n",
       " [90001, 100000]\n",
       " [100001, 110000]\n",
       " [110001, 120000]\n",
       " [120001, 130000]\n",
       " ⋮\n",
       " [540001, 550000]\n",
       " [550001, 560000]\n",
       " [560001, 570000]\n",
       " [570001, 580000]\n",
       " [580001, 590000]\n",
       " [590001, 600000]\n",
       " [600001, 610000]\n",
       " [610001, 620000]\n",
       " [620001, 630000]\n",
       " [630001, 640000]\n",
       " [640001, 650000]\n",
       " [650001, 655657]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_key_file = \"datasets/run0053-run0114-cal-analysis.txt\";\n",
    "meta_keys = CSV.read(meta_key_file);\n",
    "channels = 0:1:36\n",
    "event_step = Int(1e4)\n",
    "\n",
    "current_dir = pwd()\n",
    "ddir = \"/remote/ceph/group/gerda/data/phase2/blind/\" * dataset * \"/gen/\"\n",
    "cd(ddir)\n",
    "filenames1 = []\n",
    "filenames4 = []\n",
    "for meta_key in meta_keys[1]\n",
    "    filename1 = ddir * glob(\"tier1/ged/\" * data_type * \"/\" * split(meta_key, \"-\")[2] * \"/\" * meta_key * \"*.root\")[1]\n",
    "    filename4 = ddir * glob(\"tier4/all/\" * data_type * \"/\" * split(meta_key, \"-\")[2] * \"/\" * meta_key * \"*.root\")[1]\n",
    "    push!(filenames1, filename1)\n",
    "    push!(filenames4, filename4)\n",
    "end\n",
    "cd(current_dir)\n",
    "\n",
    "if data_type == \"phy\"\n",
    "    base_path = \"pulses/data/raw_\" * dataset * \"/\"\n",
    "elseif data_type == \"cal\"\n",
    "    base_path = \"pulses/calib/raw_\" * dataset * \"/\"\n",
    "end\n",
    "log_file = base_path * \"log.json\"\n",
    "if !isdir(base_path)\n",
    "    mkpath(base_path)\n",
    "end\n",
    "if !isfile(log_file)\n",
    "    open(log_file, \"w\") do f\n",
    "        JSON.print(f, Dict(), 4)\n",
    "    end\n",
    "    global log = Dict()\n",
    "else\n",
    "    global log = JSON.parsefile(log_file)\n",
    "end;\n",
    "\n",
    "for i in eachindex(filenames4)\n",
    "    start_t = now()\n",
    "    run = parse(Int64, split(split(filenames4[i], \"gerda-run\")[2], \"-\")[1])\n",
    "    filename1 = filenames1[i]\n",
    "    filename4 = filenames4[i]\n",
    "    \n",
    "    @info(\"Run \" * string(run) * \" | file \" * string(i) * \" of total \" * string(length(filenames4)))\n",
    "    log = JSON.parsefile(log_file)\n",
    "    if length(keys(log)) >= 1\n",
    "        estimation = 0\n",
    "        for k in keys(log)\n",
    "            estimation += log[k]\n",
    "        end\n",
    "        estimation /= length(keys(log))\n",
    "        fileinfo = stat(filename4)\n",
    "        est_t = round(fileinfo.size * estimation / 60, digits=1)\n",
    "        @info(\"Estimated time for this file: \" * string(est_t) * \" min\")\n",
    "        total_filesize = 0\n",
    "        for f in i:length(filenames4)\n",
    "            total_filesize += stat(filenames4[f]).size\n",
    "        end\n",
    "        est_t = total_filesize * estimation / 60\n",
    "        unit = \" s\"\n",
    "        if est_t/60/60/24 > 1\n",
    "            est_t /= 60*60*24\n",
    "            unit = \" d\"\n",
    "        elseif est_t/60/60 > 1\n",
    "            est_t /= 60*60\n",
    "            unit = \" h\"\n",
    "        elseif est_t/60 > 1\n",
    "            est_t /= 60\n",
    "            unit = \" min\"\n",
    "        end\n",
    "        @info(\"Estimated time till completion: \" * string(round(est_t, digits=1)) * unit)\n",
    "    end\n",
    "    @info(\"------------------------------\")\n",
    "    \n",
    "\n",
    "    if true#!(filename4 in keys(JSON.parsefile(log_file)))\n",
    "        @info(\"Load Tier4\")\n",
    "        @time tier4 = Table(TFile(filename4)[\"tier4\"]);\n",
    "        temp_E = sum.(tier4.energy[:])\n",
    "\n",
    "        index_1 = findall(x->x == 1, tier4.multiplicity[:])\n",
    "        index_2 = findall(y->y > 300, temp_E[index_1])\n",
    "        index_3 = findall(x->x == 0, tier4.isBL[index_1[index_2]])\n",
    "        index_4 = findall(x->x == 0, tier4.isTP[index_1[index_2[index_3]]])\n",
    "        filtered_index = index_1[index_2[index_3[index_4]]]\n",
    "        if length(filtered_index) > 0\n",
    "            @info(\"Number of events: \" * string(length(temp_E)))\n",
    "            @info(\"Number of events after filter: \" * string(length(filtered_index)))\n",
    "            steps = []\n",
    "            s = 1\n",
    "            while s <= length(filtered_index)\n",
    "                a = s\n",
    "                b = s + event_step - 1 <= length(filtered_index) ? s+event_step-1 : length(filtered_index)\n",
    "#                 println(string(filtered_index[a]) * \" - \" * string(filtered_index[b]))\n",
    "                push!(steps, [a,b])\n",
    "                s += event_step\n",
    "            end\n",
    "            return steps\n",
    "            for s in steps\n",
    "                result = Table( energy       = [],\n",
    "                        run          = [],\n",
    "                        channel      = [],\n",
    "                        AEvetoed     = [],\n",
    "                        datasetID    = [],\n",
    "                        AEclassifier = [],\n",
    "                        waveform     = [])\n",
    "                IJulia.clear_output(true)\n",
    "                run_str = split(basename(filename4), \"-\")[2]\n",
    "                @info(\"Run \" * run_str * \" | file \" * string(i) * \" of total \" * string(length(filenames4)))\n",
    "                @info(\"Step \" * string(findfirst(x->x == s, steps)) * \" of \" * string(length(steps)))\n",
    "                file = base_path * run_str * \"/\"\n",
    "                if !isdir(file)\n",
    "                    mkpath(file)\n",
    "                end\n",
    "                file *= basename(filename4) * lpad(findfirst(x->x == s, steps), 4, \"0\") * \".h5\"\n",
    "                if !isfile(file)\n",
    "                    \n",
    "                    tmp = filtered_index[s[1]:s[2]]\n",
    "                    @info(\"Event \" * string(tmp[1]) * \" until \" * string(tmp[end]))\n",
    "                    tier4 = Table(TFile(filename4)[\"tier4\"])[tmp];\n",
    "                    @info(\"Load Tier1\")\n",
    "                    @time treeTier1 = TFile(filename1)[\"MGTree\"].event;\n",
    "                    @info(\"Apply first filter to Tier1 & Tier4\")\n",
    "                    @time waveforms = TypedTables.Table(raw2mgtevent.(treeTier1[tmp])).fAuxWaveforms\n",
    "        #             waveforms = waveforms[tmp]\n",
    "                    if length(tmp) > 0\n",
    "                        run_str = split(basename(filename4), \"-\")[2]\n",
    "                        run = parse(Int64, split(run_str, \"run\")[2])\n",
    "\n",
    "                        pro = Progress(length(channels), dt=0.5,\n",
    "                            barglyphs=BarGlyphs('|','█', ['▁' ,'▂' ,'▃' ,'▄' ,'▅' ,'▆', '▇'],' ','|',),\n",
    "                            barlen=10);\n",
    "                        for ch in channels\n",
    "\n",
    "                            tmp = findall(y->y[ch + 1] != 0, tier4.energy)\n",
    "                            if length(tmp) > 0\n",
    "                                waveform = []\n",
    "                                for i in tmp\n",
    "                                    if data_type == \"phy\"\n",
    "                                        push!(waveform, waveforms[i][ch + 1].wf)\n",
    "                                    else\n",
    "                                        push!(waveform, waveforms[i][1].wf)\n",
    "                                    end\n",
    "                                end\n",
    "\n",
    "                                AEvetoed     = []\n",
    "                                datasetID    = []\n",
    "                                AEclassifier = []\n",
    "                                for i in tmp\n",
    "                                    push!(AEvetoed,     tier4[i].isAoEvetoed[ch+1])\n",
    "                                    push!(datasetID,    tier4[i].datasetID[ch+1])\n",
    "                                    push!(AEclassifier, tier4[i].AoEclassifier[ch+1])\n",
    "                                end\n",
    "\n",
    "                                append!(result, Table(  energy       = sum.(tier4.energy[tmp]),\n",
    "                                                        run          = Int.(zeros(length(tmp)) .+ run),\n",
    "                                                        channel      = Int.(zeros(length(tmp)) .+ ch),\n",
    "                                                        AEvetoed     = AEvetoed,\n",
    "                                                        datasetID    = datasetID,\n",
    "                                                        AEclassifier = AEclassifier,\n",
    "                                                        waveform     = StructArray{RDWaveform}(Array{RDWaveform,1}(waveform))))\n",
    "                            end\n",
    "                            next!(pro)\n",
    "                        end\n",
    "                    end\n",
    "                    if size(result,1) > 0\n",
    "                        file = base_path * run_str * \"/\"\n",
    "                        file *= basename(filename4) * lpad(findfirst(x->x == s, steps), 4, \"0\") * \".h5\"\n",
    "                        HDF5.h5open(file, \"w\") do h5f\n",
    "                            LegendHDF5IO.writedata( h5f, \"data\", Table( energy       = float.(result.energy),\n",
    "                                                                        run          = Int.(result.run),\n",
    "                                                                        channel      = Int.(result.channel),\n",
    "                                                                        AEvetoed     = Int.(result.AEvetoed),\n",
    "                                                                        datasetID    = Int.(result.datasetID),\n",
    "                                                                        AEclassifier = float.(result.AEclassifier),\n",
    "                                                                        waveform     = StructArray{RDWaveform}(Array{RDWaveform,1}(result.waveform))))\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "            log = JSON.parsefile(log_file)\n",
    "            dt = now() - start_t\n",
    "            fileinfo = stat(filenames4[1])\n",
    "            fileinfo.size\n",
    "            log[filename4] = (dt.value / 1000) / fileinfo.size\n",
    "            open(log_file, \"w\") do f\n",
    "                JSON.print(f, log, 4)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    IJulia.clear_output(true)\n",
    "#     Base.run(`clear`)\n",
    "    log = JSON.parsefile(log_file)\n",
    "    @info(string(length(log)) * \" of \" * string(length(filenames4)) * \" done!\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_key_file = \"datasets/run0053-run0114-cal-analysis.txt\";\n",
    "meta_keys = CSV.read(meta_key_file);\n",
    "channels = 0:1:36\n",
    "event_step = Int(1e4)\n",
    "\n",
    "current_dir = pwd()\n",
    "ddir = \"/remote/ceph/group/gerda/data/phase2/blind/\" * dataset * \"/gen/\"\n",
    "cd(ddir)\n",
    "filenames1 = []\n",
    "filenames4 = []\n",
    "for meta_key in meta_keys[1]\n",
    "    filename1 = ddir * glob(\"tier1/ged/\" * data_type * \"/\" * split(meta_key, \"-\")[2] * \"/\" * meta_key * \"*.root\")[1]\n",
    "    filename4 = ddir * glob(\"tier4/all/\" * data_type * \"/\" * split(meta_key, \"-\")[2] * \"/\" * meta_key * \"*.root\")[1]\n",
    "    push!(filenames1, filename1)\n",
    "    push!(filenames4, filename4)\n",
    "end\n",
    "cd(current_dir)\n",
    "if data_type == \"phy\"\n",
    "    base_path = \"pulses/data/raw_\" * dataset * \"/\"\n",
    "elseif data_type == \"cal\"\n",
    "    base_path = \"pulses/calib/raw_\" * dataset * \"/\"\n",
    "end\n",
    "log_file = base_path * \"log.json\"\n",
    "if !isdir(base_path)\n",
    "    mkpath(base_path)\n",
    "end\n",
    "if !isfile(log_file)\n",
    "    open(log_file, \"w\") do f\n",
    "        JSON.print(f, Dict(), 4)\n",
    "    end\n",
    "    global log = Dict()\n",
    "else\n",
    "    global log = JSON.parsefile(log_file)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.030328 seconds (42.46 k allocations: 2.506 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99.29626197055194"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(stat(filenames1[1]).size)/1024/1024\n",
    "process_speed = ((stat(filenames1[1]).size)/1024/1024) / (242 * 60)\n",
    "total_size = 0\n",
    "@time (for f in filenames1\n",
    "    total_size += (stat(f).size)/1024/1024\n",
    "end)\n",
    "(total_size / process_speed) / 3600 / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.034966 seconds (136 allocations: 5.328 KiB)\n",
      "  0.166404 seconds (150.24 k allocations: 62.017 MiB, 5.46% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time treeTier1 = TFile(filenames1[1])[\"MGTree\"].event;\n",
    "@time waveforms = TypedTables.Table(raw2mgtevent.(treeTier1[1:1000])).fAuxWaveforms;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin\n",
    "   lock(a)\n",
    "   try\n",
    "       use(a)\n",
    "   finally\n",
    "       unlock(a)\n",
    "   end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "TaskFailedException:\nMethodError: no method matching lock(::var\"#39#42\"{String}, ::Nothing)\nClosest candidates are:\n  lock(::Any, !Matched::Base.GenericCondition) at condition.jl:78\n  lock(::Any, !Matched::Base.AbstractLock) at lock.jl:158\n  lock(::Any, !Matched::WeakKeyDict) at weakkeydict.jl:76\nStacktrace:\n [1] macro expansion at ./In[8]:3 [inlined]\n [2] (::var\"#404#threadsfor_fun#41\"{Array{Any,1}})(::Bool) at ./threadingconstructs.jl:81\n [3] (::var\"#404#threadsfor_fun#41\"{Array{Any,1}})() at ./threadingconstructs.jl:48",
     "output_type": "error",
     "traceback": [
      "TaskFailedException:\nMethodError: no method matching lock(::var\"#39#42\"{String}, ::Nothing)\nClosest candidates are:\n  lock(::Any, !Matched::Base.GenericCondition) at condition.jl:78\n  lock(::Any, !Matched::Base.AbstractLock) at lock.jl:158\n  lock(::Any, !Matched::WeakKeyDict) at weakkeydict.jl:76\nStacktrace:\n [1] macro expansion at ./In[8]:3 [inlined]\n [2] (::var\"#404#threadsfor_fun#41\"{Array{Any,1}})(::Bool) at ./threadingconstructs.jl:81\n [3] (::var\"#404#threadsfor_fun#41\"{Array{Any,1}})() at ./threadingconstructs.jl:48",
      "",
      "Stacktrace:",
      " [1] wait at ./task.jl:267 [inlined]",
      " [2] threading_run(::Function) at ./threadingconstructs.jl:34",
      " [3] top-level scope at ./threadingconstructs.jl:93",
      " [4] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "Threads.@threads for f in filenames1[1:3]\n",
    "    println(string(Threads.threadid()))\n",
    "    lock(treeTier1) do \n",
    "        treeTier1 = TFile(f)[\"MGTree\"].event;\n",
    "        @time waveforms = TypedTables.Table(raw2mgtevent.(treeTier1[1:1000])).fAuxWaveforms\n",
    "        waveform = []\n",
    "        for wf in waveforms\n",
    "            push!(waveform, wf.wf[1])\n",
    "        end\n",
    "        HDF5.h5open(\"testing/\" * basename(f) * \".h5\", \"w\") do h5f\n",
    "            LegendHDF5IO.writedata(h5f, \"data\", Table(waveform = StructArray{RDWaveform}(Array{RDWaveform,1}(waveform))))\n",
    "        end\n",
    "        waveforms = nothing\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{StepRange{Int64,Int64},1}:\n",
       " 1:1:1000\n",
       " 1001:1:2000\n",
       " 2001:1:3000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [1:1:1000, 1001:1:2000, 2001:1:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "TaskFailedException:\nMethodError: no method matching lock(::var\"#32#36\"{StepRange{Int64,Int64}}, ::TBranch)\nClosest candidates are:\n  lock(::Any, !Matched::Base.GenericCondition) at condition.jl:78\n  lock(::Any, !Matched::Base.AbstractLock) at lock.jl:158\n  lock(::Any, !Matched::WeakKeyDict) at weakkeydict.jl:76\nStacktrace:\n [1] macro expansion at ./In[5]:4 [inlined]\n [2] (::var\"#343#threadsfor_fun#35\"{Array{StepRange{Int64,Int64},1}})(::Bool) at ./threadingconstructs.jl:81\n [3] (::var\"#343#threadsfor_fun#35\"{Array{StepRange{Int64,Int64},1}})() at ./threadingconstructs.jl:48",
     "output_type": "error",
     "traceback": [
      "TaskFailedException:\nMethodError: no method matching lock(::var\"#32#36\"{StepRange{Int64,Int64}}, ::TBranch)\nClosest candidates are:\n  lock(::Any, !Matched::Base.GenericCondition) at condition.jl:78\n  lock(::Any, !Matched::Base.AbstractLock) at lock.jl:158\n  lock(::Any, !Matched::WeakKeyDict) at weakkeydict.jl:76\nStacktrace:\n [1] macro expansion at ./In[5]:4 [inlined]\n [2] (::var\"#343#threadsfor_fun#35\"{Array{StepRange{Int64,Int64},1}})(::Bool) at ./threadingconstructs.jl:81\n [3] (::var\"#343#threadsfor_fun#35\"{Array{StepRange{Int64,Int64},1}})() at ./threadingconstructs.jl:48",
      "",
      "Stacktrace:",
      " [1] wait at ./task.jl:267 [inlined]",
      " [2] threading_run(::Function) at ./threadingconstructs.jl:34",
      " [3] top-level scope at ./threadingconstructs.jl:93",
      " [4] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "f = filenames1[1]\n",
    "treeTier1 = TFile(f)[\"MGTree\"].event;\n",
    "Threads.@threads for s in steps\n",
    "    lock(treeTier1) do\n",
    "        local waveforms = TypedTables.Table(raw2mgtevent.(treeTier1[s])).fAuxWaveforms\n",
    "        local waveform = []\n",
    "        for wf in waveforms\n",
    "            push!(waveform, wf.wf[1])\n",
    "        end\n",
    "        HDF5.h5open(\"testing/\" * basename(f) * string(findfirst(x->x==s, steps)) * \".h5\", \"w\") do h5f\n",
    "            LegendHDF5IO.writedata(h5f, \"data\", Table(waveform = StructArray{RDWaveform}(Array{RDWaveform,1}(waveform))))\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Distributed; \n",
    "addprocs(4);\n",
    "nprocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/remote/ceph/group/gerda/data/phase2/blind/v07.01/gen/tier1/ged/cal/run0053/gerda-run0053-20151223T105042Z-cal-ged-tier1.root\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task (failed) @0x00007fdd23b24280\n",
       "\u001b[91mInterruptException:\u001b[39m\n",
       "sigatomic_end at ./c.jl:425 [inlined]\n",
       "disable_sigint at ./c.jl:448 [inlined]\n",
       "__pycall! at /user/.julia/packages/PyCall/BcTLp/src/pyfncall.jl:42 [inlined]\n",
       "_pycall!(::PyCall.PyObject, ::PyCall.PyObject, ::Tuple{}, ::Int64, ::PyCall.PyObject) at /user/.julia/packages/PyCall/BcTLp/src/pyfncall.jl:29\n",
       "_pycall!(::PyCall.PyObject, ::PyCall.PyObject, ::Tuple{}, ::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:entrystart, :entrystop),Tuple{Int64,Int64}}}) at /user/.julia/packages/PyCall/BcTLp/src/pyfncall.jl:11\n",
       "#_#117 at /user/.julia/packages/PyCall/BcTLp/src/pyfncall.jl:86 [inlined]\n",
       "getindex(::TBranch, ::UnitRange{Int64}) at /user/.julia/packages/UpROOT/pN5xO/src/ttree.jl:127\n",
       "getindex(::TBranch, ::Int64) at /user/.julia/packages/UpROOT/pN5xO/src/ttree.jl:131\n",
       "hash(::TBranch, ::UInt64) at ./abstractarray.jl:2297\n",
       "hash(::TBranch) at ./hashing.jl:18\n",
       "serialize_global_from_main(::Distributed.ClusterSerializer{Sockets.TCPSocket}, ::Symbol) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/clusterserialize.jl:171\n",
       "#6 at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/clusterserialize.jl:101 [inlined]\n",
       "foreach at ./abstractarray.jl:2009 [inlined]\n",
       "serialize(::Distributed.ClusterSerializer{Sockets.TCPSocket}, ::Core.TypeName) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/clusterserialize.jl:101\n",
       "serialize_type_data(::Distributed.ClusterSerializer{Sockets.TCPSocket}, ::DataType) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Serialization/src/Serialization.jl:530\n",
       "serialize(::Distributed.ClusterSerializer{Sockets.TCPSocket}, ::DataType) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Serialization/src/Serialization.jl:567\n",
       "serialize_type_data(::Distributed.ClusterSerializer{Sockets.TCPSocket}, ::DataType) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Serialization/src/Serialization.jl:550\n",
       "serialize_type(::Distributed.ClusterSerializer{Sockets.TCPSocket}, ::DataType, ::Bool) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Serialization/src/Serialization.jl:574\n",
       "serialize_any(::Distributed.ClusterSerializer{Sockets.TCPSocket}, ::Any) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Serialization/src/Serialization.jl:644\n",
       "serialize at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Serialization/src/Serialization.jl:627 [inlined]\n",
       "serialize_msg(::Distributed.ClusterSerializer{Sockets.TCPSocket}, ::Distributed.CallMsg{:call}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/messages.jl:90\n",
       "#invokelatest#1 at ./essentials.jl:710 [inlined]\n",
       "invokelatest at ./essentials.jl:709 [inlined]\n",
       "send_msg_(::Distributed.Worker, ::Distributed.MsgHeader, ::Distributed.CallMsg{:call}, ::Bool) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/messages.jl:185\n",
       "send_msg at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/messages.jl:134 [inlined]\n",
       "#remotecall#140 at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/remotecall.jl:365 [inlined]\n",
       "remotecall(::Function, ::Distributed.Worker) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/remotecall.jl:364\n",
       "remotecall(::Function, ::Int64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/remotecall.jl:376\n",
       "remotecall at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/remotecall.jl:376 [inlined]\n",
       "spawnat(::Int64, ::Function) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/macros.jl:15\n",
       "spawn_somewhere(::Function) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/macros.jl:17\n",
       "macro expansion at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/macros.jl:95 [inlined]\n",
       "(::Distributed.var\"#159#161\"{var\"#81#82\",Array{StepRange{Int64,Int64},1}})() at ./task.jl:332"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = filenames1[1]\n",
    "treeTier1 = TFile(f)[\"MGTree\"].event;\n",
    "@distributed for s in steps\n",
    "    @time waveforms = TypedTables.Table(raw2mgtevent.(treeTier1[s])).fAuxWaveforms\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    local waveform = []\n",
    "    for wf in waveforms\n",
    "        push!(waveform, wf.wf[1])\n",
    "    end\n",
    "#     HDF5.h5open(\"testing/\" * basename(f) * string(findfirst(x->x==s, steps)) * \".h5\", \"w\") do h5f\n",
    "#         LegendHDF5IO.writedata(h5f, \"data\", Table(waveform = StructArray{RDWaveform}(Array{RDWaveform,1}(waveform))))\n",
    "#     end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration & splitting into detectors (Sep 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type == \"phy\"\n",
    "    plots_base_path = \"pulses/data/raw_\" * dataset * \"/plots/\"\n",
    "    base_path_raw   = \"pulses/data/raw_\" * dataset * \"/\"\n",
    "    base_path       = \"pulses/data/\" * dataset * \"/\"\n",
    "elseif data_type == \"cal\"\n",
    "    plots_base_path = \"pulses/calib/raw_\" * dataset * \"/plots/\"\n",
    "    base_path_raw   = \"pulses/calib/raw_\" * dataset * \"/\"\n",
    "    base_path       = \"pulses/calib/\" * dataset * \"/\"\n",
    "end\n",
    "number_of_pulses = 5e3\n",
    "bl_range = 1:1:200\n",
    "cal = JSON.parsefile(\"data_pulses/FEP_pulses/calibration/cal.json\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(base_path_raw * \"*/*.h5\");\n",
    "data_cal = Dict()\n",
    "for ch in 0:1:36\n",
    "    data_cal[string(ch)] = Dict()\n",
    "    data_cal[string(ch)][\"data\"] = Table(energy = [], run = [], channel = [], AoEvetoed = [], datasetID = [], AoEclassifier = [], A = [], E = [], waveform = [])\n",
    "    data_cal[string(ch)][\"counter\"] = 1\n",
    "end\n",
    "pro = Progress(length(files), dt=0.5,\n",
    "                barglyphs=BarGlyphs('|','█', ['▁' ,'▂' ,'▃' ,'▄' ,'▅' ,'▆', '▇'],' ','|',),\n",
    "                barlen=10);\n",
    "for file in files\n",
    "    if stat(file).size > 1000\n",
    "        data = HDF5.h5open(file, \"r\") do h5f\n",
    "            LegendHDF5IO.readdata(h5f, \"data\")\n",
    "        end;\n",
    "        for ch in unique(data.channel)\n",
    "            ch_str = lpad(ch, 2, \"0\");\n",
    "            temp = data |> @filter(_.channel == ch) |> Table\n",
    "            cal_factor = cal[string(ch)][\"value\"]\n",
    "            waveforms = []\n",
    "            A = []\n",
    "            E = []\n",
    "            for wf in temp.waveform\n",
    "                pulse   = -1 .*float.(wf.value)\n",
    "                pulse .-= sum(pulse[bl_range])/length(bl_range)\n",
    "                pulse .*= cal_factor\n",
    "                push!(A, maximum(multi_mwa(diff(pulse),5,3)))\n",
    "                push!(E, maximum(multi_mwa(diff(pulse),201,6)))\n",
    "                push!(waveforms, RDWaveform(wf.time, pulse))\n",
    "            end\n",
    "            \n",
    "            append!(data_cal[string(ch)][\"data\"], Table(energy   = temp.energy,\n",
    "                                                        run      = temp.run,\n",
    "                                                        channel  = temp.channel,\n",
    "                                                        AoEvetoed= temp.AEvetoed,\n",
    "                                                        datasetID= temp.datasetID,\n",
    "                                                        AoEclassifier = temp.AEclassifier,\n",
    "                                                        A        = A,\n",
    "                                                        E        = E,\n",
    "                                                        waveform = waveforms))\n",
    "            if size(data_cal[string(ch)][\"data\"],1) >= number_of_pulses\n",
    "                filename = base_path * channel_to_name[ch] * \"-\" * ch_str * \"/\"\n",
    "                !isdir(filename) ? mkpath(filename) : \"path exists\"\n",
    "                filename *= string(data_cal[string(ch)][\"counter\"]) * \".h5\"\n",
    "                data_cal[string(ch)][\"counter\"] += 1\n",
    "                HDF5.h5open(filename, \"w\") do h5f\n",
    "                    LegendHDF5IO.writedata( h5f, \"data\", Table( energy       = float.(data_cal[string(ch)][\"data\"].energy),\n",
    "                                                                run          = Int.(data_cal[string(ch)][\"data\"].run),\n",
    "                                                                channel      = Int.(data_cal[string(ch)][\"data\"].channel),\n",
    "                                                                AoEvetoed     = Int.(data_cal[string(ch)][\"data\"].AoEvetoed),\n",
    "                                                                datasetID    = Int.(data_cal[string(ch)][\"data\"].datasetID),\n",
    "                                                                AoEclassifier= float.(data_cal[string(ch)][\"data\"].AoEclassifier),\n",
    "                                                                A            = float.(data_cal[string(ch)][\"data\"].A),\n",
    "                                                                E            = float.(data_cal[string(ch)][\"data\"].E),\n",
    "                                                                waveform     = StructArray{RDWaveform}(Array{RDWaveform,1}(data_cal[string(ch)][\"data\"].waveform))))\n",
    "                end\n",
    "                data_cal[string(ch)][\"data\"] = Table(energy = [], run = [], channel = [], AoEvetoed = [], datasetID = [], AoEclassifier = [], A = [], E = [], waveform = [])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "#     IJulia.clear_output(true)\n",
    "#     @info(string(findfirst(x->x == file, files)) * \" of \" * string(length(files)))\n",
    "#     for ch in keys(data_cal)\n",
    "#         ch_str = lpad(parse(Int64, ch), 2, \"0\");\n",
    "#         println(\"Ch\" * ch_str * \" | \" * string(size(data_cal[ch][\"data\"],1)))\n",
    "#     end\n",
    "    next!(pro)\n",
    "end\n",
    "for ch in 0:1:36\n",
    "    ch_str = lpad(ch, 2, \"0\");\n",
    "    if size(data_cal[string(ch)][\"data\"],1) > 0\n",
    "        filename = base_path * channel_to_name[ch] * \"-\" * ch_str * \"/\"\n",
    "        !isdir(filename) ? mkpath(filename) : \"path exists\"\n",
    "        filename *= string(data_cal[string(ch)][\"counter\"]) * \".h5\"\n",
    "        data_cal[string(ch)][\"counter\"] += 1\n",
    "        HDF5.h5open(filename, \"w\") do h5f\n",
    "            LegendHDF5IO.writedata( h5f, \"data\", Table( energy       = float.(data_cal[string(ch)][\"data\"].energy),\n",
    "                                                        run          = Int.(data_cal[string(ch)][\"data\"].run),\n",
    "                                                        channel      = Int.(data_cal[string(ch)][\"data\"].channel),\n",
    "                                                        AoEvetoed     = Int.(data_cal[string(ch)][\"data\"].AoEvetoed),\n",
    "                                                        datasetID    = float.(data_cal[string(ch)][\"data\"].datasetID),\n",
    "                                                        AoEclassifier= float.(data_cal[string(ch)][\"data\"].AoEclassifier),\n",
    "                                                        A            = float.(data_cal[string(ch)][\"data\"].A),\n",
    "                                                        E            = float.(data_cal[string(ch)][\"data\"].E),\n",
    "                                                        waveform     = StructArray{RDWaveform}(Array{RDWaveform,1}(data_cal[string(ch)][\"data\"].waveform))))\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get calibration constant for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
